{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_5-Gy5ZA_Jh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1:What is Ensemble Learning in machine learning? Explain the key idea\n",
        "behind it.\n",
        "Answer:Ensemble learning is a machine learning technique that combines the prediction of multiple models which is more stable and accurate as compared to individual models.\n",
        "The key idea behind ensemble learning combines the strenghts of multiple models to produce a better prediction."
      ],
      "metadata": {
        "id": "bOIcrJQSBRVO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the difference between Bagging and Boosting?\n",
        "Answer:Bagging is an ensemble technique that:\n",
        "1.Creates multiple subsets of the training data using random sampling with replacement (bootstrapping).\n",
        "2.Trains a model on each subset of data.\n",
        "3.Combines the predictions of all models using voting or averaging.\n",
        "Boosting is an ensemble technique that:\n",
        "1.Trains a model on the entire training data.\n",
        "2.Trains another model on the weighted data, focusing on the errors of the previous model.\n",
        "3.Combines the predictions of all models"
      ],
      "metadata": {
        "id": "GszOnYZyKb80"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is bootstrap sampling and what role does it play in Bagging methods\n",
        "like Random Forest?\n",
        "Answer: Bootstrap sampling plays a role in Bagging methods like Random Forest:\n",
        "1.Bootstrap sampling is used to create multiple subsets of the original dataset.\n",
        "2.Each subset of data is used to train a decision tree in the Random Forest ensemble.\n",
        "3.Random forest reduces the variance(overfitting).\n"
      ],
      "metadata": {
        "id": "MLTAuj8FLZJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models?\n",
        "Answer:OOB samples are part of  train data ,not used in model training for individual  models/decision tree.\n",
        "To evaluate the OOB score one has to  take the average of all individual OOB score."
      ],
      "metadata": {
        "id": "HWTKHTPBMlw7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: Compare feature importance analysis in a single Decision Tree vs. a\n",
        "Random Forest.\n",
        "Answer:The differences between feature importance analysis in a single Decision Tree vs. a Random Forest are:\n",
        "1.Stability: Random Forests provide more stable feature importance score. Decision Trees can be sensitive to the specific data and splits.\n",
        "2.Robustness: Random Forests are more robust to overfitting and noise in the data, which can affect feature importance scores.\n",
        "3.Feature interactions: Random Forests can capture complex feature interactions and relationships, than Decision trees.\n",
        "\n"
      ],
      "metadata": {
        "id": "Na2M7XuMNTLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 6: Write a Python program to:\n",
        "#sklearn.datasets.load_breast_cancer()\n",
        "#● Train a Random Forest Classifier\n",
        "#● Print the top 5 most important features based on feature importance scores\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "feature_names = breast_cancer.feature_names\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=1)\n",
        "rf.fit(X_train, y_train)\n",
        "feature_importances = rf.feature_importances_\n",
        "feature_importances_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
        "feature_importances_df = feature_importances_df.sort_values(by='Importance', ascending=False)\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "print(feature_importances_df.head(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmOtcnjGOrCC",
        "outputId": "503e5b56-205b-4da0-c8de-a108e98bca31"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Most Important Features:\n",
            "                 Feature  Importance\n",
            "22       worst perimeter    0.130773\n",
            "27  worst concave points    0.130307\n",
            "23            worst area    0.117678\n",
            "7    mean concave points    0.091731\n",
            "20          worst radius    0.084314\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 7: Write a Python program to:\n",
        "#● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "#● Evaluate its accuracy and compare with a single Decision Tree\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "dt = DecisionTreeClassifier( )\n",
        "dt.fit(X_train, y_train)\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
        "print(\"Accuracy of single Decision Tree:\", accuracy_dt)\n",
        "bagging = BaggingClassifier(best_estimator_=DecisionTreeClassifier(x,y), n_estimators=10, random_state=42)\n",
        "bagging.fit(X_train, y_train)\n",
        "y_pred_bagging = bagging.predict(X_test)\n",
        "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
        "print(\"Accuracy of Bagging Classifier:\", accuracy_bagging)\n",
        "print(\"Improvement in accuracy:\", accuracy_bagging - accuracy_dt)\n",
        "if accuracy_bagging > accuracy_dt:\n",
        "    print(\"Bagging Classifier performs better than single Decision Tree.\")\n",
        "else:\n",
        "    print(\"Single Decision Tree performs better than Bagging Classifier.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "E13FRkj_QdKK",
        "outputId": "ba7e7ffd-81a4-4bc2-9964-f6234ee18001"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of single Decision Tree: 0.9666666666666667\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'x' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1051540049.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0maccuracy_dt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_dt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy of single Decision Tree:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_dt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mbagging\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBaggingClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mbagging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0my_pred_bagging\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbagging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 8: Write a Python program to:\n",
        "#● Train a Random Forest Classifier\n",
        "#● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "#● Print the best parameters and final accuracy\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "param_grid = {\n",
        "    'max_depth': [None, 5, 10, 15],\n",
        "    'n_estimators': [10, 50, 100, 200]\n",
        "}\n",
        "rf = RandomForestClassifier(random_state=2)\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "best_rf = RandomForestClassifier(**grid_search.best_params_, random_state=2)\n",
        "best_rf.fit(X_train, y_train)\n",
        "y_pred = best_rf.predict(X_test)\n",
        "final_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Final Accuracy:\", final_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wH55PELdC0D",
        "outputId": "6a858d90-2714-4c95-d12b-fe5047f49c9d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'n_estimators': 200}\n",
            "Final Accuracy: 0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 9\n",
        "#● Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "#Housing dataset\n",
        "#● Compare their Mean Squared Errors (MSE)\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "cal_housing = fetch_california_housing()\n",
        "X = cal_housing.data\n",
        "y = cal_housing.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "bagging = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=10, random_state=42)\n",
        "bagging.fit(X_train, y_train)\n",
        "y_pred_bagging = bagging.predict(X_test)\n",
        "mse_bagging = mean_squared_error(y_test, y_pred_bagging)\n",
        "print(\"MSE of Bagging Regressor:\", mse_bagging_regressor)\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "print(\"MSE of Random Forest Regressor:\", mse_rf)\n",
        "print(\"Difference in MSE:\", mse_bagging - mse_rf)\n",
        "if mse_bagging > mse_rf:\n",
        "    print(\"Random Forest Regressor performs better than Bagging Regressor.\")\n",
        "else:\n",
        "    print(\"Bagging Regressor performs better than Random Forest Regressor.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "2nrkX_rmeAFW",
        "outputId": "c6813fbf-9d0a-4bc7-dd04-8dbe7e8704f9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'mse_bagging_regressor' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-994984449.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0my_pred_bagging\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbagging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mmse_bagging\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_bagging\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MSE of Bagging Regressor:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse_bagging_regressor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mrf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'mse_bagging_regressor' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 10: You are working as a data scientist at a financial institution to predict loan\n",
        "#default. You have access to customer demographic and transaction history data.\n",
        "#You decide to use ensemble techniques to increase model performance.\n",
        "#Explain your step-by-step approach to:\n",
        "#● Choose between Bagging or Boosting\n",
        "#● Handle overfitting\n",
        "#● Select base models\n",
        "#● Evaluate performance using cross-validation\n",
        "#● Justify how ensemble learning improves decision-making in this real-world\n",
        "#context\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import xgboost as xgb\n",
        "from sklearn.datasets import make_classification\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=3, n_repeated=2, n_classes=2, random_state=2)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
        "smote = SMOTE(random_state=2)\n",
        "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
        "scaler = StandardScaler()\n",
        "X_train_res = scaler.fit_transform(X_train_res)\n",
        "X_test = scaler.transform(X_test_res)\n",
        "bagging = BaggingClassifier(n_estimators=10, random_state=2)\n",
        "bagging.fit(X_train_res, y_train_res)\n",
        "y_pred_bagging = bagging.predict(X_test)\n",
        "print(\"Bagging Classifier:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_bagging))\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_bagging))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_bagging))\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train_res, y_train_res)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "print(\"Random Forest Classifier:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_rf))\n",
        "adaboost = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
        "adaboost.fit(X_train_res, y_train_res)\n",
        "y_pred_adaboost = adaboost.predict(X_test)\n",
        "print(\"AdaBoost Classifier:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_adaboost))\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_adaboost))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_adaboost))\n",
        "xgb_clf = xgb.XGBClassifier(objective='binary:logistic', n_estimators=50, random_state=42)\n",
        "xgb_clf.fit(X_train_res, y_train_res)\n",
        "y_pred_xgb = xgb_clf.predict(X_test)\n",
        "print(\"XGBoost Classifier:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_xgb))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_xgb))\n",
        "param_grid = {\n",
        "    'n_estimators': [10, 50, 100],\n",
        "    'max_depth': [None, 5, 10]\n",
        "}\n",
        "grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5)\n",
        "grid_search.fit(X_train_res, y_train_res)\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Score:\", grid_search.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "iCfkN_K_ez3R",
        "outputId": "6369e117-e34f-44be-ad11-042beb2b9ec6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_test_res' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-232117389.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mX_train_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mbagging\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBaggingClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mbagging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_test_res' is not defined"
          ]
        }
      ]
    }
  ]
}